---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: prometheus
  namespace: monitoring
spec:
  interval: 15m
  chart:
    spec:
      chart: prometheus
      version: 15.12.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  # https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
  values:
    serviceAccounts:
      alertmanager:
        create: true
        name:
        annotations: {}
      nodeExporter:
        create: true
        name:
        annotations: {}
      pushgateway:
        create: true
        name:
        annotations: {}
      server:
        create: true
        name:
        annotations: {}
    alertmanager:
      replicaCount: 1
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: nginx
        hosts:
          - &host "alert-manager.${PUBLIC_DOMAIN}"
        tls:
          - hosts:
              - *host
      securityContext:
        runAsUser: "${SECURITY_CONTEXT_RUN_AS_USER}"
        runAsGroup: "${SECURITY_CONTEXT_RUN_AS_GROUP}"
        fsGroup: "${SECURITY_CONTEXT_FS_GROUP}"
        runAsNonRoot: false
      persistentVolume:
        enabled: true
        accessModes: [ReadWriteMany]
        mountPath: /data
        existingClaim: appdata
        subPath: prometheus/alertmanager
    alertmanagerFiles:
      alertmanager.yml:
        receivers:
          - name: "null"
          - name: email
            email_configs:
              - send_resolved: false
                to: "${NOTIFY_EMAIL}"
                from: "AlertManager <${SMTP_USER}>"
                smarthost: maddy.default.svc.cluster.local:2525
                require_tls: false
        route:
          group_by: [alertname, job]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 6h
          receiver: email
          routes:
            - receiver: "null"
              matchers:
                - alertname =~ "InfoInhibitor|Watchdog"
            - receiver: email
              matchers:
                - severity = "critical"
              continue: true
        inhibit_rules:
          - source_matchers:
              - severity = "critical"
            target_matchers:
              - severity = "warning"
            equal: [alertname, namespace]
    kube-state-metrics:
      enabled: true
      # metricLabelsAllowlist:
      #   - "persistentvolumeclaims=[*]"
      # prometheus:
      #   monitor:
      #     enabled: true
      #     relabelings:
      #       - action: replace
      #         regex: (.*)
      #         replacement: $1
      #         sourceLabels:
      #           - __meta_kubernetes_pod_node_name
      #         targetLabel: kubernetes_node
    nodeExporter:
      enabled: true
    server:
      name: prometheus
      statefulSet:
        enabled: false
      extraArgs:
        log.level: debug
        # https://github.com/thanos-io/thanos/blob/0d659bf171afa6bdf5c5ece3033df3a7e8245d8c/tutorials/kubernetes-helm/README.md
        storage.tsdb.min-block-duration: 2h
        storage.tsdb.max-block-duration: 2h
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: nginx
        annotations:
          external-dns.home.arpa/enabled: "true"
        hosts:
          - &host "prometheus.${PUBLIC_DOMAIN}"
        tls:
          - hosts:
              - *host
      securityContext:
        runAsUser: "${SECURITY_CONTEXT_RUN_AS_USER}"
        runAsGroup: "${SECURITY_CONTEXT_RUN_AS_GROUP}"
        fsGroup: "${SECURITY_CONTEXT_FS_GROUP}"
        runAsNonRoot: false
      persistentVolume:
        enabled: true
        accessModes: [ReadWriteMany]
        mountPath: /data
        existingClaim: appdata
        subPath: prometheus/server
    # ### Start Thanos ###
    # # https://github.com/thanos-io/thanos/blob/0d659bf171afa6bdf5c5ece3033df3a7e8245d8c/tutorials/kubernetes-helm/README.md
    #   configPath: /etc/prometheus-shared/prometheus.yml
    #   extraVolumes:
    #     - name: prometheus-config-shared
    #       emptyDir: {}
    #   extraVolumeMounts:
    #     - name: prometheus-config-shared
    #       mountPath: /etc/prometheus-shared/
    #   global:
    #     scrape_interval: 5s
    #     scrape_timeout: 4s
    #     external_labels:
    #       prometheus_group: "${CLUSTER_NAME}"
    #       prometheus_replica: '$(HOSTNAME)'
    #     evaluation_interval: 5s
    #   # extraSecretMounts:
    #   #   - name: thanos-storage-secret
    #   #     mountPath: /etc/secret/
    #   #     subPath: sa
    #   #     readOnly: false
    #   #     secretName: thanos
    #   service:
    #     gRPC:
    #       enabled: true
    #     annotations:
    #       prometheus.io/scrape: "true"
    #       prometheus.io/port: "9090"
    #   podAnnotations:
    #     prometheus.io/scrape: "true"
    #     prometheus.io/port: "10902"
    #     secret.reloader.stakater.com/reload: thanos
    #   sidecarContainers:
    #     thanos-sidecar:
    #       # https://quay.io/repository/thanos/thanos?tab=tags
    #       image: quay.io/thanos/thanos:v0.28.0
    #       env:
    #         # https://thanos.io/tip/thanos/storage.md/#s3
    #         - name: AWS_ACCESS_KEY_ID
    #           valueFrom:
    #             secretKeyRef:
    #               name: thanos-minio
    #               key: MINIO_ROOT_USER
    #               optional: false
    #         - name: AWS_SECRET_ACCESS_KEY
    #           valueFrom:
    #             secretKeyRef:
    #               name: thanos-minio
    #               key: MINIO_ROOT_PASSWORD
    #               optional: false
    #       args:
    #         - "sidecar"
    #         - "--log.level=debug"
    #         - "--tsdb.path=/data/"
    #         - "--prometheus.url=http://127.0.0.1:9090"
    #         # https://thanos.io/tip/thanos/storage.md/#s3
    #         - |
    #           --objstore.config={
    #             type: S3,
    #             config: {
    #               bucket: thanos,
    #               endpoint: thanos-minio.monitoring.svc.cluster.local:9000,
    #               insecure: true
    #             }
    #           }
    #         - "--reloader.config-file=/etc/prometheus-config/prometheus.yml"
    #         - "--reloader.config-envsubst-file=/etc/prometheus-shared/prometheus.yml"
    #         - "--reloader.rule-dir=/etc/prometheus-config/rules"
    #       ports:
    #         - name: sidecar-http
    #           containerPort: 10902
    #         - name: grpc
    #           containerPort: 10901
    #         - name: cluster
    #           containerPort: 10900
    #       volumeMounts:
    #         - name: storage-volume
    #           mountPath: /data
    #         - name: config-volume
    #           mountPath: /etc/prometheus-config
    #           readOnly: false
    #         - name: prometheus-config-shared
    #           mountPath: /etc/prometheus-shared/
    #           readOnly: false
    # configmapReload:
    #   image:
    #     # This image changed to just pause since there's no option to
    #     # disable configmapReload container in chart, but thanos-sidecar
    #     # overtakes this functionality. So basically we don't need another reloader
    #     repository: gcr.io/google-containers/pause-amd64
    #     tag: 3.1
    #   # resources:
    #   #   limits:
    #   #     cpu: 20m
    #   #     memory: 20Mi
    #   #   requests:
    #   #     cpu: 20m
    #   #     memory: 20Mi
    # ### End Thanos ###
    pushgateway:
      securityContext:
        runAsUser: "${SECURITY_CONTEXT_RUN_AS_USER}"
        runAsGroup: "${SECURITY_CONTEXT_RUN_AS_GROUP}"
        fsGroup: "${SECURITY_CONTEXT_FS_GROUP}"
        runAsNonRoot: false
      persistentVolume:
        enabled: true
        mountPath: /data
        existingClaim: appdata
        subPath: prometheus/pushgateway
    extraScrapeConfigs: |
      - job_name: node-exporter
        scrape_interval: 1m
        scrape_timeout: 10s
        honor_timestamps: true
        static_configs:
          - targets:
              - "opnsense.${PRIVATE_DOMAIN}:9100"

# ---
# apiVersion: helm.toolkit.fluxcd.io/v2beta1
# kind: HelmRelease
# metadata:
#   name: thanos-minio
#   namespace: monitoring
# spec:
#   interval: 15m
#   chart:
#     spec:
#       chart: app-template
#       version: 0.2.1
#       sourceRef:
#         kind: HelmRepository
#         name: bjw-s
#         namespace: flux-system
#   values:
#     image:
#       repository: quay.io/minio/minio
#       tag: RELEASE.2022-09-07T22-25-02Z
#     env:
#       TZ: "${TIMEZONE}"
#       MINIO_UPDATE: "off"
#       MINIO_SERVER_URL: "https://thanos-s3.${PUBLIC_DOMAIN}"
#       MINIO_PROMETHEUS_URL: http://prometheus-prometheus.monitoring.svc.cluster.local:9090
#       MINIO_PROMETHEUS_JOB_ID: minio
#       MINIO_BROWSER_REDIRECT_URL: "https://minio.${PUBLIC_DOMAIN}"
#       MINIO_IDENTITY_OPENID_CONFIG_URL: "https://auth.${PUBLIC_DOMAIN}/.well-known/openid-configuration"
#       MINIO_IDENTITY_OPENID_CLIENT_ID: thanos-minio
#       MINIO_IDENTITY_OPENID_CLIENT_SECRET: "${THANOS_MINIO_OAUTH_CLIENT_SECRET}"
#       MINIO_IDENTITY_OPENID_SCOPES: "openid,profile,email"
#       MINIO_IDENTITY_OPENID_REDIRECT_URI: "https://thanos-s3.${PUBLIC_DOMAIN}/oauth_callback"
#     envFrom:
#       - secretRef:
#           name: thanos-minio
#     args:
#       - server
#       - /data
#       - --address
#       - :9000
#       - --console-address
#       - :9001
#     service:
#       main:
#         enabled: true
#         ports:
#           http:
#             enabled: true
#             port: 9001
#           api:
#             enabled: true
#             port: 9000
#     ingress:
#       main:
#         enabled: true
#         ingressClassName: nginx
#         annotations:
#           hajimari.io/enable: "true"
#           hajimari.io/icon: pail
#         hosts:
#           - host: &host "thanos-s3.${PUBLIC_DOMAIN}"
#             paths:
#               - path: /
#                 pathType: Prefix
#         tls:
#           - hosts:
#               - *host
#     podSecurityContext:
#       runAsUser: "${SECURITY_CONTEXT_RUN_AS_USER}"
#       runAsGroup: "${SECURITY_CONTEXT_RUN_AS_GROUP}"
#       fsGroup: "${SECURITY_CONTEXT_FS_GROUP}"
#     persistence:
#       data:
#         enabled: true
#         existingClaim: appdata
#         mountPath: /data
#         subPath: thanos_s3
#     podAnnotations:
#       secret.reloader.stakater.com/reload: thanos
#     # resources:
#     #   requests:
#     #     cpu: 22m
#     #     memory: 1500M
#     #   limits:
#     #     # cpu: 49m
#     #     memory: 2000M
